# Eventbrite Dublin Events Data Pipeline

This project is an end-to-end **data analysis pipeline** built around real Eventbrite event data for Dublin.
It covers the full journey from **raw data → cleaned dataset → engineered features → visual analysis → database queries**.

The main goal of this project is to demonstrate **practical data engineering and analytics skills** using messy, real-world data — not a perfectly clean Kaggle dataset.

---

## What this project actually does

At a high level, the pipeline has **three core stages**, plus analysis:

1. **Data acquisition (optional)**
2. **Data cleaning**
3. **Feature engineering**
4. **Visualization & database queries**
5. **Testing & validation**

You can run **only the parts you need**.
If you don’t want to scrape live data, you can skip acquisition and work directly with the processed CSV files.

---

## Project structure

```
eventbrite-dublin-events-pipeline/
│
├── config/
│   ├── config_template.py      # Public template (safe to commit)
│   └── config_local.py         # Your personal session info (DO NOT COMMIT)
│
├── data/
│   ├── raw/                    # Raw JSON from Eventbrite
│   ├── processed/              # Cleaned & engineered CSV files
│   └── db/                     # SQLite database
│
├── charts/
│   └── outputs/                # PNG charts generated by the pipeline
│
├── src/
│   ├── data_acquisition.py
│   ├── data_cleaning.py
│   ├── feature_engineering.py
│   ├── charts.py
│   ├── load_db.py
│   └── test.py
│
├── notebooks/                  # Original exploratory notebook
├── requirements.txt
└── README.md
```

---

## Data acquisition

**This step is optional.**

The repository already contains raw json file as the output of this part.
You only need to run data acquisition **if you want to fetch fresh Eventbrite data yourself**.

### If you want to fetch live data

1. Copy:

   ```
   config/config_template.py
   ```
2. Rename it to:

   ```
   config/config_local.py
   ```
3. Open Eventbrite in your browser
4. Go to **DevTools → Network → destination/search**
5. Copy:

   * `stable_id`
   * request headers
   * cookies
6. Paste them into `config_local.py`

If you don’t do this step, simply skip `data_acquisition.py`.

---

## How to run the pipeline

All scripts are designed to be run from the **project root** using `-m`:

```bash
python -m src.data_acquisition      # optional
python -m src.data_cleaning
python -m src.feature_engineering
python -m src.charts
python -m src.load_db
python -m src.test
```

Using `-m` avoids import issues and is the recommended way to run the project.

---

## Data cleaning

The cleaning stage focuses on **making the data usable**, not adding new meaning.

Main steps:

* Flatten nested JSON
* Remove fully empty columns
* Extract city, neighbourhood, venue address
* Normalize timestamps
* Handle missing values
* Remove duplicates

Output:

```
data/processed/events_clean_base.csv
```

---

## Feature engineering

This stage adds **analytical meaning** to the cleaned data.

Examples of engineered features:

* Event duration
* Lead time (days published before event)
* Weekend / night indicators
* Price categories (free / low / medium / high)
* High-level event category flags
* Eircode extraction
* Dublin area clustering
* Address normalization

Output:

```
data/processed/final_dataset.csv
```

This dataset is designed to be:

* Easy to analyze
* Ready for SQL
* Suitable for ML or dashboards

---

## Visual analysis

The `charts.py` script generates PNG charts and saves them to:

```
charts/outputs/
```

Examples:

* Event category distribution
* Ticket price vs weekday
* Event density heatmap (day × hour)
* Lead time comparison (free vs paid)
* Top venues by event count

No interactive notebooks are required to reproduce the visuals.

---

## Database & queries

The pipeline loads the final dataset into **SQLite**:

```
data/db/events.db
```

Example queries include:

* Total number of events
* Most expensive weekend events
* Event distribution by month

This mirrors how analytics pipelines often support both **Python and SQL users**.

---

## Testing & validation

Basic tests are included to make sure logic didn’t silently break:

* Unit tests:

  * Price categorization
  * Address parsing
* Integration test:

  * Confirm data is written to SQLite
* Sanity checks:

  * Duplicate IDs
  * Missing values

Tests can be run with:

```bash
python -m src.test
```

---

## License

MIT
